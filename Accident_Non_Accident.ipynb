{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project,we're going to buil a accident detector for Twitter tweets using the multinomial Naive Bayes algorithm.Our goal is to write a progam that classifies new tweets with an accuracy greater than 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the algorithm,we'll use a dataset of 7,613 Twitter tweets that are already classified by humans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweet = pd.read_csv('accident.csv')\n",
    "\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text - the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                      61\n",
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "sinking                  41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 222, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['keyword'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['keyword'] = tweet['keyword'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                             2533\n",
       "USA                              104\n",
       "New York                          71\n",
       "United States                     50\n",
       "London                            45\n",
       "                                ... \n",
       "Surulere Lagos,Home Of Swagg       1\n",
       "MontrÌ©al, QuÌ©bec                 1\n",
       "Montreal                           1\n",
       "ÌÏT: 6.4682,3.18287                1\n",
       "Lincoln                            1\n",
       "Name: location, Length: 3342, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['location'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop 'location' and 'id' and 'keyword' column because it isn't useful for our Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweet.drop(['location','id','keyword'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target -this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['target'] = tweet['target'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_replace ={\n",
    "     'target':{\n",
    "         '1' : 'accident',\n",
    "         '0' : 'non_accident',\n",
    "     }\n",
    "}\n",
    "tweet = tweet.replace(status_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non_accident    0.57034\n",
       "accident        0.42966\n",
       "Name: target, dtype: Float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that about 57% of the tweet are non_accident and the remaining are accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to split our dataset into training and a test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 2)\n",
      "(1523, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomize the dataset\n",
    "data_randomized = tweet.sample(frac=1, random_state=1)\n",
    "\n",
    "# Calculate index for split\n",
    "training_test_index = round(len(data_randomized) * 0.8)\n",
    "\n",
    "# Training/Test split\n",
    "training_set = data_randomized[:training_test_index].reset_index(drop=True)\n",
    "test_set = data_randomized[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goulburn man henry van bilsen missing: emergen...</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the things we fear most in organizations--fluc...</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@tsunami_esh ?? hey esh</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@potus you until you drown by water entering t...</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crawling in my skin\\nthese wounds they will no...</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        target\n",
       "0  goulburn man henry van bilsen missing: emergen...      accident\n",
       "1  the things we fear most in organizations--fluc...  non_accident\n",
       "2                            @tsunami_esh ?? hey esh  non_accident\n",
       "3  @potus you until you drown by water entering t...  non_accident\n",
       "4  crawling in my skin\\nthese wounds they will no...      accident"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['text'] = training_set['text'].str.lower()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Vacabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['text'] = training_set['text'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for sms in training_set['text']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23731"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Final Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training_set['text']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(training_set['text']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@ladyfleur</th>\n",
       "      <th>grew</th>\n",
       "      <th>mistreated.</th>\n",
       "      <th>happening!</th>\n",
       "      <th>ten-4.</th>\n",
       "      <th>#japan</th>\n",
       "      <th>legionnaire's</th>\n",
       "      <th>150+</th>\n",
       "      <th>body-bagging</th>\n",
       "      <th>http://t.co/gyzpisbi1u</th>\n",
       "      <th>...</th>\n",
       "      <th>steamship</th>\n",
       "      <th>clerical</th>\n",
       "      <th>http://t.co/vg7jnah0iw</th>\n",
       "      <th>trail</th>\n",
       "      <th>û÷exceptionalûª</th>\n",
       "      <th>melanie</th>\n",
       "      <th>http://t.co/te2yerugsi</th>\n",
       "      <th>gillibrand</th>\n",
       "      <th>fatally</th>\n",
       "      <th>http://t.co/jmkywhv7mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23731 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   @ladyfleur  grew  mistreated.  happening!  ten-4.  #japan  legionnaire's  \\\n",
       "0           0     0            0           0       0       0              0   \n",
       "1           0     0            0           0       0       0              0   \n",
       "2           0     0            0           0       0       0              0   \n",
       "3           0     0            0           0       0       0              0   \n",
       "4           0     0            0           0       0       0              0   \n",
       "\n",
       "   150+  body-bagging  http://t.co/gyzpisbi1u  ...  steamship  clerical  \\\n",
       "0     0             0                       0  ...          0         0   \n",
       "1     0             0                       0  ...          0         0   \n",
       "2     0             0                       0  ...          0         0   \n",
       "3     0             0                       0  ...          0         0   \n",
       "4     0             0                       0  ...          0         0   \n",
       "\n",
       "   http://t.co/vg7jnah0iw  trail  û÷exceptionalûª  melanie  \\\n",
       "0                       0      0                  0        0   \n",
       "1                       0      0                  0        0   \n",
       "2                       0      0                  0        0   \n",
       "3                       0      0                  0        0   \n",
       "4                       0      0                  0        0   \n",
       "\n",
       "   http://t.co/te2yerugsi  gillibrand  fatally  http://t.co/jmkywhv7mp  \n",
       "0                       0           0        0                       0  \n",
       "1                       0           0        0                       0  \n",
       "2                       0           0        0                       0  \n",
       "3                       0           0        0                       0  \n",
       "4                       0           0        0                       0  \n",
       "\n",
       "[5 rows x 23731 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>@ladyfleur</th>\n",
       "      <th>grew</th>\n",
       "      <th>mistreated.</th>\n",
       "      <th>happening!</th>\n",
       "      <th>ten-4.</th>\n",
       "      <th>#japan</th>\n",
       "      <th>legionnaire's</th>\n",
       "      <th>150+</th>\n",
       "      <th>...</th>\n",
       "      <th>steamship</th>\n",
       "      <th>clerical</th>\n",
       "      <th>http://t.co/vg7jnah0iw</th>\n",
       "      <th>trail</th>\n",
       "      <th>û÷exceptionalûª</th>\n",
       "      <th>melanie</th>\n",
       "      <th>http://t.co/te2yerugsi</th>\n",
       "      <th>gillibrand</th>\n",
       "      <th>fatally</th>\n",
       "      <th>http://t.co/jmkywhv7mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[goulburn, man, henry, van, bilsen, missing:, ...</td>\n",
       "      <td>accident</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[the, things, we, fear, most, in, organization...</td>\n",
       "      <td>non_accident</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[@tsunami_esh, ??, hey, esh]</td>\n",
       "      <td>non_accident</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[@potus, you, until, you, drown, by, water, en...</td>\n",
       "      <td>non_accident</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[crawling, in, my, skin, these, wounds, they, ...</td>\n",
       "      <td>accident</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23733 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        target  \\\n",
       "0  [goulburn, man, henry, van, bilsen, missing:, ...      accident   \n",
       "1  [the, things, we, fear, most, in, organization...  non_accident   \n",
       "2                       [@tsunami_esh, ??, hey, esh]  non_accident   \n",
       "3  [@potus, you, until, you, drown, by, water, en...  non_accident   \n",
       "4  [crawling, in, my, skin, these, wounds, they, ...      accident   \n",
       "\n",
       "   @ladyfleur  grew  mistreated.  happening!  ten-4.  #japan  legionnaire's  \\\n",
       "0         0.0   0.0          0.0         0.0     0.0     0.0            0.0   \n",
       "1         0.0   0.0          0.0         0.0     0.0     0.0            0.0   \n",
       "2         0.0   0.0          0.0         0.0     0.0     0.0            0.0   \n",
       "3         0.0   0.0          0.0         0.0     0.0     0.0            0.0   \n",
       "4         0.0   0.0          0.0         0.0     0.0     0.0            0.0   \n",
       "\n",
       "   150+  ...  steamship  clerical  http://t.co/vg7jnah0iw  trail  \\\n",
       "0   0.0  ...        0.0       0.0                     0.0    0.0   \n",
       "1   0.0  ...        0.0       0.0                     0.0    0.0   \n",
       "2   0.0  ...        0.0       0.0                     0.0    0.0   \n",
       "3   0.0  ...        0.0       0.0                     0.0    0.0   \n",
       "4   0.0  ...        0.0       0.0                     0.0    0.0   \n",
       "\n",
       "   û÷exceptionalûª  melanie  http://t.co/te2yerugsi  gillibrand  fatally  \\\n",
       "0                0.0      0.0                     0.0         0.0      0.0   \n",
       "1                0.0      0.0                     0.0         0.0      0.0   \n",
       "2                0.0      0.0                     0.0         0.0      0.0   \n",
       "3                0.0      0.0                     0.0         0.0      0.0   \n",
       "4                0.0      0.0                     0.0         0.0      0.0   \n",
       "\n",
       "   http://t.co/jmkywhv7mp  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "\n",
       "[5 rows x 23733 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_clean = training_set_clean.loc[:,~training_set_clean.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below,we'll use our training set to calculate:\n",
    "P(Accident) and P(Non_accident),NAccident,N_Non_Accident,N_Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating accident and non_accident tweets first\n",
    "accident_tweet = training_set_clean.copy()[training_set_clean['target'] == 'accident']\n",
    "non_accident_tweet = training_set_clean.copy()[training_set_clean['target'] == 'non_accident']\n",
    "\n",
    "# P(Accident) and P(Non_accident)\n",
    "p_accident = len(accident_tweet) / len(training_set_clean) #will use later #\n",
    "p_non_accident = len(non_accident_tweet) / len(training_set_clean)   #will use later #\n",
    "\n",
    "# N_Accident\n",
    "n_words_per_accident_tweet = accident_tweet['text'].apply(len)\n",
    "n_accident = n_words_per_accident_tweet.sum()\n",
    "\n",
    "# N_Non_Accident\n",
    "n_words_per_non_accident_tweet= non_accident_tweet['target'].apply(len)\n",
    "n_non_accident = n_words_per_non_accident_tweet.sum()\n",
    "\n",
    "# N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1 = accident_tweet.columns.drop(['text','target'])\n",
    "columns_2 = non_accident_tweet.columns.drop(['text','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_tweet = accident_tweet.dropna()\n",
    "non_accident_tweet = non_accident_tweet.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameters\n",
    "parameters_accident = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_non_accident = {unique_word:0 for unique_word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_1:\n",
    "    n_word_given_accident = accident_tweet[column].sum()   \n",
    "    p_word_given_accident = (n_word_given_accident + alpha) / (n_accident + alpha*n_vocabulary)\n",
    "    parameters_accident[column] = p_word_given_accident\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_2:\n",
    "    n_word_given_non_accident = non_accident_tweet[column].sum()   \n",
    "    p_word_given_non_accident = (n_word_given_non_accident + alpha) / (n_non_accident + alpha*n_vocabulary)\n",
    "    parameters_non_accident[column] = p_word_given_non_accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying A New Message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "    \n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    p_accident_given_message = p_accident\n",
    "    p_non_accident_given_message = p_non_accident\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_accident:\n",
    "            p_accident_given_message *= parameters_accident[word]\n",
    "            \n",
    "        if word in parameters_non_accident:\n",
    "            p_non_accident_given_message *= parameters_non_accident[word]\n",
    "            \n",
    "    print('P(Accident|message):', p_accident_given_message)\n",
    "    print('P(Non_accident|message):', p_non_accident_given_message)\n",
    "    \n",
    "    if p_non_accident_given_message > p_accident_given_message:\n",
    "        print('Label: Non_accident')\n",
    "    elif p_non_accident_given_message < p_accident_given_message:\n",
    "        print('Label: Accident')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@DwarfOnJetpack I guess I can say you and me m...</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E1.1.2 Particulate=Break up of Solid Combust F...</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.Beyonce Is my pick for http://t.co/thoYhrHkf...</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@KirCut1 lets get a dope picture together and ...</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast milk is the original #superfood but rat...</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        target\n",
       "0  @DwarfOnJetpack I guess I can say you and me m...  non_accident\n",
       "1  E1.1.2 Particulate=Break up of Solid Combust F...      accident\n",
       "2  7.Beyonce Is my pick for http://t.co/thoYhrHkf...  non_accident\n",
       "3  @KirCut1 lets get a dope picture together and ...  non_accident\n",
       "4  Breast milk is the original #superfood but rat...  non_accident"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.loc[:,~test_set.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):    \n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "    \n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    p_accident_given_message = p_accident\n",
    "    p_non_accident_given_message = p_non_accident\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_accident:\n",
    "            p_accident_given_message *= parameters_accident[word]\n",
    "            \n",
    "        if word in parameters_non_accident:\n",
    "            p_non_accident_given_message *= parameters_non_accident[word]\n",
    "    \n",
    "    if p_non_accident_given_message > p_accident_given_message:\n",
    "        return 'non_accident'\n",
    "    elif p_accident_given_message > p_non_accident_given_message:\n",
    "        return 'accident'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@DwarfOnJetpack I guess I can say you and me m...</td>\n",
       "      <td>non_accident</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E1.1.2 Particulate=Break up of Solid Combust F...</td>\n",
       "      <td>accident</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.Beyonce Is my pick for http://t.co/thoYhrHkf...</td>\n",
       "      <td>non_accident</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@KirCut1 lets get a dope picture together and ...</td>\n",
       "      <td>non_accident</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast milk is the original #superfood but rat...</td>\n",
       "      <td>non_accident</td>\n",
       "      <td>non_accident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        target  \\\n",
       "0  @DwarfOnJetpack I guess I can say you and me m...  non_accident   \n",
       "1  E1.1.2 Particulate=Break up of Solid Combust F...      accident   \n",
       "2  7.Beyonce Is my pick for http://t.co/thoYhrHkf...  non_accident   \n",
       "3  @KirCut1 lets get a dope picture together and ...  non_accident   \n",
       "4  Breast milk is the original #superfood but rat...  non_accident   \n",
       "\n",
       "      predicted  \n",
       "0  non_accident  \n",
       "1  non_accident  \n",
       "2  non_accident  \n",
       "3  non_accident  \n",
       "4  non_accident  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['predicted'] = test_set['text'].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 862\n",
      "Incorrect: 661\n",
      "Accuracy: 0.5659881812212738\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = test_set.shape[0]\n",
    "    \n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['target'] == row['predicted']:\n",
    "        correct += 1\n",
    "        \n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is close to 60%,which reach our accuracy target but isn't really good."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
